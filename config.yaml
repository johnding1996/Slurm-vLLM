# vLLM Slurm Job Configuration

1:
  model_name: "neuralmagic/Qwen2-0.5B-Instruct-FP8"
  number_of_jobs: 3
  gpu: "gpu:rtxa4000:1"

2:
  model_name: "Qwen/Qwen2.5-0.5B-Instruct"
  number_of_jobs: 1
  gpu: "gpu:rtxa4000:1"

3:
  model_name: "neuralmagic/Llama-3.2-1B-Instruct-FP8"
  number_of_jobs: 1
  gpu: "gpu:rtxa4000:1" 